{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('keras': conda)",
   "metadata": {
    "interpreter": {
     "hash": "9f54b5b20bb07ba0d67c3b3c7af35980bc40b562dd1f8d08779883d401b670c3"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Mask classifier\n",
    "\n",
    "3 classes:\n",
    "- 0: NO PERSON in the image is wearing a mask\n",
    "- 1: ALL THE PEOPLE in the image are wearing a mask\n",
    "- 2: SOMEONE in the image is not wearing a mask\"\n",
    "\n",
    "For more details see [https://www.kaggle.com/c/artificial-neural-networks-and-deep-learning-2020]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed to make results reproducible\n",
    "SEED = 1234\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "cwd = Path.cwd()"
   ]
  },
  {
   "source": [
    "## Preprocessing\n",
    "\n",
    "Prior to running this notebook one should have prepared the dataset folders by using `prepare_dataset.py`, which creates a handout set for validation from the original training images, by extracing a certain percentage from each directory.\n",
    "\n",
    "The script is expected to find a `MaskDataset` folder as extracted from the provided zip file, and will create the structure expected by `flow_from_directory`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = cwd.joinpath('MaskDataset')\n",
    "train_dir = dataset_dir.joinpath('training')\n",
    "valid_dir = dataset_dir.joinpath('validation')\n",
    "\n",
    "class_names = ['NO_MASK', 'ALL_MASK', 'SOME_MASK']"
   ]
  },
  {
   "source": [
    "Set up the image data generators for automatic augmentation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APPLY_AUGMENTATION = True\n",
    "\n",
    "if APPLY_AUGMENTATION:\n",
    "    train_data_gen = ImageDataGenerator(\n",
    "        rotation_range=10,\n",
    "        width_shift_range=10,\n",
    "        height_shift_range=10,\n",
    "        brightness_range=(0.75,1.25), # 0 is black, 1 is original image\n",
    "        # shear_range=10, # in degrees\n",
    "        zoom_range=.3, # % of size\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        rescale=1/255.\n",
    "    )\n",
    "else:\n",
    "    train_data_gen = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "# Do not perform augmentation on validation set\n",
    "valid_data_gen = ImageDataGenerator(rescale=1/255.)"
   ]
  },
  {
   "source": [
    "Load data from disk and split it in batches"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "TARGET_SIZE = (224,224)\n",
    "\n",
    "flow_from_directory_kwargs = dict(\n",
    "    target_size=TARGET_SIZE,\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "train_gen = train_data_gen.flow_from_directory(train_dir, **flow_from_directory_kwargs)\n",
    "valid_gen = valid_data_gen.flow_from_directory(valid_dir, shuffle=False, **flow_from_directory_kwargs)\n",
    "\n",
    "from_generator_kwargs = dict(\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=((None, *TARGET_SIZE, 3), (None, len(class_names)))\n",
    ")\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(lambda: train_gen, **from_generator_kwargs)\n",
    "valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, **from_generator_kwargs)\n",
    "\n",
    "train_dataset = train_dataset.repeat()\n",
    "valid_dataset = valid_dataset.repeat()"
   ]
  },
  {
   "source": [
    "Show some (augmented) images from the first training batch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "images, labels = next(iter(train_dataset))\n",
    "for i in range(8):\n",
    "    ax = plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.title(class_names[np.argmax(labels[i])])\n",
    "    plt.axis('off')"
   ]
  },
  {
   "source": [
    "## Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Set up the callbacks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for our models\n",
    "models_dir = cwd.joinpath('alexnetlike')\n",
    "Path.mkdir(models_dir, exist_ok=True)\n",
    "\n",
    "def gen_callbacks(model_name):\n",
    "    ENABLE_CHECKPOINT = True\n",
    "    ENABLE_TENSORBOARD = True\n",
    "    ENABLE_EARLYSTOP = True\n",
    "    callbacks = []\n",
    "\n",
    "    # Create directory for this particular model\n",
    "    model_dir = models_dir.joinpath(model_name)\n",
    "    Path.mkdir(model_dir, exist_ok=False)\n",
    "\n",
    "    # Checkpointing callback\n",
    "    if ENABLE_CHECKPOINT:\n",
    "        ckpt_dir = model_dir.joinpath('ckpts')\n",
    "        Path.mkdir(ckpt_dir, exist_ok=True)\n",
    "\n",
    "        callbacks.append(keras.callbacks.ModelCheckpoint(\n",
    "            str(model_dir.joinpath('ckpts','ckpt_{epoch:02d}.hdf5')),\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True\n",
    "        ))\n",
    "\n",
    "    # Tensorboard callback\n",
    "    if ENABLE_TENSORBOARD:\n",
    "        tb_dir = model_dir.joinpath('logs')\n",
    "        Path.mkdir(tb_dir, exist_ok=True)\n",
    "\n",
    "        callbacks.append(keras.callbacks.TensorBoard(\n",
    "            log_dir=str(tb_dir),\n",
    "            histogram_freq=1,\n",
    "            update_freq='epoch',\n",
    "            profile_batch=0\n",
    "        ))\n",
    "\n",
    "    # Early stop callback\n",
    "    if ENABLE_EARLYSTOP:\n",
    "        early_stop = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            min_delta=1e-2,\n",
    "            verbose=1,\n",
    "            patience=10  # min. number of epochs of non-improving in order to stop\n",
    "        )\n",
    "        callbacks.append(early_stop)\n",
    "\n",
    "    return callbacks"
   ]
  },
  {
   "source": [
    "Define the model using the Sequential API"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_model(model_name):\n",
    "\n",
    "    KERNEL_REGULARIZER = None\n",
    "    ENABLE_DROPOUT = True\n",
    "    DROPOUT_RATE_CONV = 0.2\n",
    "    DROPOUT_RATE_DENSE = 0.5\n",
    "\n",
    "    CONV_PADDING='same'\n",
    "\n",
    "    model = keras.Sequential(name=model_name)\n",
    "    # Input layer\n",
    "    model.add(keras.Input(shape=(*TARGET_SIZE,3)))\n",
    "\n",
    "    # Convolutional layers\n",
    "    # Parameters are (n_filters, kernel_size)\n",
    "    volumes = [(16, (3,3)), (32, (3,3)), (64, (3,3)), (128, (3,3)), (256, (3,3))]\n",
    "    for i, (filters, kernel_size) in enumerate(volumes):\n",
    "        # 2D convolution layer(s)\n",
    "        model.add(tf.keras.layers.Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=CONV_PADDING,\n",
    "            activation='relu',\n",
    "            kernel_regularizer=KERNEL_REGULARIZER,\n",
    "            name=f'conv2d_{i+1}_1'\n",
    "        ))\n",
    "\n",
    "        # MaxPooling layer\n",
    "        model.add(tf.keras.layers.MaxPool2D(\n",
    "            pool_size=(2,2),\n",
    "            name=f'maxpool2d_{i+1}'\n",
    "        ))\n",
    "\n",
    "        if ENABLE_DROPOUT:\n",
    "            model.add(keras.layers.Dropout(DROPOUT_RATE_CONV, name=f'dropout_conv_{i+1}'))\n",
    "\n",
    "    # MLP layers\n",
    "    INITIAL_UNITS = 512\n",
    "    MLP_DEPTH = 1\n",
    "    model.add(keras.layers.Flatten(name='flatten'))\n",
    "\n",
    "    for i in range(MLP_DEPTH):\n",
    "        model.add(keras.layers.Dense(\n",
    "            INITIAL_UNITS,\n",
    "            activation='relu',\n",
    "            kernel_regularizer=KERNEL_REGULARIZER,\n",
    "            name=f'dense_{i+1}'\n",
    "        ))\n",
    "\n",
    "        if ENABLE_DROPOUT:\n",
    "            model.add(keras.layers.Dropout(DROPOUT_RATE_DENSE, name=f'dropout_dense_{i+1}'))\n",
    "\n",
    "    # Softmax layer\n",
    "    model.add(keras.layers.Dense(\n",
    "        len(class_names),\n",
    "        activation='softmax',\n",
    "        name='dense_softmax'\n",
    "    ))\n",
    "\n",
    "    # Show a summary of the model\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "source": [
    "Choose loss, learning rate, and optimizer. Then compile the model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    # We are using a categorical (i.e. one-hot output)\n",
    "    loss = keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "    # Learning rate can be adjusted\n",
    "    learning_rate = 1e-4\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    metrics = ['accuracy']\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics\n",
    "    )"
   ]
  },
  {
   "source": [
    "## Training\n",
    "\n",
    "Fit the model to data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "RUN_SUFFIX = ''\n",
    "\n",
    "model = gen_model('Mask_cnn_mlp_7')\n",
    "model_dir = models_dir.joinpath(model.name + RUN_SUFFIX)\n",
    "\n",
    "compile_model(model)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    steps_per_epoch=len(train_gen),\n",
    "    validation_steps=len(valid_gen),\n",
    "    callbacks=gen_callbacks(model.name + RUN_SUFFIX)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model configuration to file\n",
    "with open(model_dir.joinpath('model.json'), 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "\n",
    "# Save history to file\n",
    "df = pd.DataFrame(history.history)\n",
    "df.to_csv(model_dir.joinpath('metrics.csv'))"
   ]
  },
  {
   "source": [
    "## Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "If we wish we can load a pretrained model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_load = 'Mask_cnn_mlp_7'\n",
    "with open(models_dir.joinpath(model_to_load, 'model.json')) as f:\n",
    "    # Load pre-saved configuration\n",
    "    model = keras.models.model_from_json(f.read())\n",
    "    # Load weights from checkpoint\n",
    "    model.load_weights(str(models_dir.joinpath(model_to_load, 'ckpts', 'ckpt_22.hdf5')))"
   ]
  },
  {
   "source": [
    "We compute the predictions on non-augmented validation data, then log the confusion matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(labels, predictions):\n",
    "    # Compute the confusion matrix\n",
    "    cm = tf.math.confusion_matrix(labels, predictions).numpy()\n",
    "\n",
    "    # Draw the figure\n",
    "    figure = plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    # Normalize the confusion matrix.\n",
    "    cmn = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "    # Use white text if squares are dark; otherwise black.\n",
    "    threshold = cmn.max() / 2.\n",
    "    for i, j in [(i,j) for i in range(cmn.shape[0]) for j in range(cmn.shape[1])]:\n",
    "        color = \"white\" if cmn[i, j] > threshold else \"black\"\n",
    "        plt.text(j, i, cmn[i, j], horizontalalignment=\"center\", color=color)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    return figure\n",
    "\n",
    "predictions = model.predict(valid_gen)\n",
    "pred_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = valid_gen.classes\n",
    "\n",
    "figure = plot_confusion_matrix(true_classes, pred_classes)"
   ]
  },
  {
   "source": [
    "## Prediction\n",
    "\n",
    "Perform a batch prediction on the test dataset, then save the results to a file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = dataset_dir.joinpath('test')\n",
    "\n",
    "img_paths = list(test_dir.iterdir())\n",
    "\n",
    "def load_preprocess(img_path):\n",
    "    img = keras.preprocessing.image.load_img(img_path).resize(TARGET_SIZE)\n",
    "    img = keras.preprocessing.image.img_to_array(img) / 255.\n",
    "\n",
    "    return img\n",
    "\n",
    "img_batch = np.array([load_preprocess(p) for p in img_paths])\n",
    "predictions = map(np.argmax, model.predict(img_batch))\n",
    "\n",
    "results = pd.DataFrame({'Id': map(lambda p: p.name, img_paths), 'Category': predictions})\n",
    "\n",
    "results.to_csv(model_dir.joinpath('results.csv'), index=False)"
   ]
  }
 ]
}