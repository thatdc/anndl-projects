{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('keras': conda)",
   "metadata": {
    "interpreter": {
     "hash": "9f54b5b20bb07ba0d67c3b3c7af35980bc40b562dd1f8d08779883d401b670c3"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Mask classifier\n",
    "\n",
    "3 classes:\n",
    "- 0: NO PERSON in the image is wearing a mask\n",
    "- 1: ALL THE PEOPLE in the image are wearing a mask\n",
    "- 2: SOMEONE in the image is not wearing a mask\"\n",
    "\n",
    "For more details see [https://www.kaggle.com/c/artificial-neural-networks-and-deep-learning-2020]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "cwd = Path.cwd()"
   ]
  },
  {
   "source": [
    "## Preprocessing\n",
    "\n",
    "Prior to running this notebook one should have prepared the dataset folders by using `prepare_dataset.py`, which creates a handout set for validation from the original training images, by extracing a certain percentage from each directory.\n",
    "\n",
    "The script is expected to find a `MaskDataset` folder as extracted from the provided zip file, and will create the structure expected by `flow_from_directory`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = cwd.joinpath('MaskDataset')\n",
    "train_dir = dataset_dir.joinpath('training')\n",
    "valid_dir = dataset_dir.joinpath('validation')\n",
    "\n",
    "class_names = ['NO_MASK', 'ALL_MASK', 'SOME_MASK']"
   ]
  },
  {
   "source": [
    "Set up the image data generators for automatic augmentation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APPLY_AUGMENTATION = True\n",
    "preprocessing_function = keras.applications.vgg16.preprocess_input\n",
    "\n",
    "if APPLY_AUGMENTATION:\n",
    "    train_data_gen = ImageDataGenerator(\n",
    "        rotation_range=30,\n",
    "        width_shift_range=10,\n",
    "        height_shift_range=10,\n",
    "        brightness_range=(0.75,1.15), # 0 is black, 1 is original image\n",
    "        zoom_range=.15, # % of size\n",
    "        horizontal_flip=True,\n",
    "        # vertical_flip=True,\n",
    "        fill_mode='constant',\n",
    "        cval=0,\n",
    "        preprocessing_function=preprocessing_function\n",
    "    )\n",
    "else:\n",
    "    train_data_gen = ImageDataGenerator(preprocessing_function=preprocessing_function)\n",
    "\n",
    "# Do not perform augmentation on validation set\n",
    "valid_data_gen = ImageDataGenerator(preprocessing_function=preprocessing_function)"
   ]
  },
  {
   "source": [
    "Load data from disk and split it in batches"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "TARGET_SIZE = (224,224)\n",
    "\n",
    "flow_from_directory_kwargs = dict(\n",
    "    target_size=TARGET_SIZE,\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "train_gen = train_data_gen.flow_from_directory(train_dir, **flow_from_directory_kwargs)\n",
    "valid_gen = valid_data_gen.flow_from_directory(valid_dir, shuffle=False, **flow_from_directory_kwargs)\n",
    "\n",
    "from_generator_kwargs = dict(\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=((None, *TARGET_SIZE, 3), (None, len(class_names)))\n",
    ")\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(lambda: train_gen, **from_generator_kwargs)\n",
    "valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, **from_generator_kwargs)\n",
    "\n",
    "train_dataset = train_dataset.repeat()\n",
    "valid_dataset = valid_dataset.repeat()"
   ]
  },
  {
   "source": [
    "Show some (augmented) images from the first training batch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "images, labels = next(iter(train_dataset))\n",
    "for i in range(8):\n",
    "    ax = plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.title(class_names[np.argmax(labels[i])])\n",
    "    plt.axis('off')"
   ]
  },
  {
   "source": [
    "## Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Set up the callbacks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for our models\n",
    "models_dir = cwd.joinpath('vgg')\n",
    "Path.mkdir(models_dir, exist_ok=True)\n",
    "\n",
    "def gen_callbacks(model_name):\n",
    "    ENABLE_CHECKPOINT = True\n",
    "    ENABLE_TENSORBOARD = True\n",
    "    ENABLE_EARLYSTOP = False\n",
    "    ENABLE_REDUCE_LR = False\n",
    "    callbacks = []\n",
    "\n",
    "    # Create directory for this particular model\n",
    "    model_dir = models_dir.joinpath(model_name)\n",
    "    Path.mkdir(model_dir, exist_ok=False)\n",
    "\n",
    "    # Checkpointing callback\n",
    "    if ENABLE_CHECKPOINT:\n",
    "        ckpt_dir = model_dir.joinpath('ckpts')\n",
    "        Path.mkdir(ckpt_dir, exist_ok=True)\n",
    "\n",
    "        callbacks.append(keras.callbacks.ModelCheckpoint(\n",
    "            str(model_dir.joinpath('ckpts','ckpt_{epoch:02d}.hdf5')),\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True\n",
    "        ))\n",
    "\n",
    "    # Tensorboard callback\n",
    "    if ENABLE_TENSORBOARD:\n",
    "        tb_dir = model_dir.joinpath('logs')\n",
    "        Path.mkdir(tb_dir, exist_ok=True)\n",
    "\n",
    "        callbacks.append(keras.callbacks.TensorBoard(\n",
    "            log_dir=str(tb_dir),\n",
    "            histogram_freq=1,\n",
    "            update_freq='epoch',\n",
    "            profile_batch=0\n",
    "        ))\n",
    "\n",
    "    # Early stop callback\n",
    "    if ENABLE_EARLYSTOP:\n",
    "        early_stop = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            min_delta=1e-2,\n",
    "            verbose=1,\n",
    "            patience=10  # min. number of epochs of non-improving in order to stop\n",
    "        )\n",
    "        callbacks.append(early_stop)\n",
    "\n",
    "    # Reduce learning rate on plateau\n",
    "    if ENABLE_REDUCE_LR:\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=np.sqrt(.1),\n",
    "            verbose=1,\n",
    "            patience=5,\n",
    "            cooldown=0,\n",
    "            min_lr=.5e-6\n",
    "        )\n",
    "        callbacks.append(reduce_lr)\n",
    "    return callbacks"
   ]
  },
  {
   "source": [
    "Define the model using the Sequential API"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_model(model_name):\n",
    "\n",
    "    KERNEL_REGULARIZER = None\n",
    "    ENABLE_DROPOUT = True\n",
    "    DROPOUT_RATE_CONV = 0.2\n",
    "    DROPOUT_RATE_DENSE = 0.5\n",
    "    ENABLE_BATCH_NORMALIZATION = False\n",
    "\n",
    "    CONV_PADDING='same'\n",
    "\n",
    "    # Instantiate VGG16 network without top classifier\n",
    "    base_model = keras.applications.VGG16(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg',\n",
    "        input_tensor=keras.Input(shape=(*TARGET_SIZE,3))\n",
    "    )\n",
    "    # Freeze the model to perform transfer learning\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Input of classifier head\n",
    "    head = keras.Sequential(name='mlp_head')\n",
    "    head.add(keras.Input(shape=base_model.output_shape[1:]))\n",
    "\n",
    "    if ENABLE_BATCH_NORMALIZATION:\n",
    "        head.add(keras.layers.BatchNormalization(name='batchnorm_flatten'))\n",
    "\n",
    "    # MLP layers\n",
    "    INITIAL_UNITS = 64\n",
    "    MLP_DEPTH = 0\n",
    "\n",
    "    for i in range(MLP_DEPTH):\n",
    "        head.add(keras.layers.Dense(\n",
    "            INITIAL_UNITS,\n",
    "            activation='relu',\n",
    "            kernel_regularizer=KERNEL_REGULARIZER,\n",
    "            name=f'dense_{i+1}'\n",
    "        ))\n",
    "\n",
    "        if ENABLE_DROPOUT:\n",
    "            head.add(keras.layers.Dropout(DROPOUT_RATE_DENSE, name=f'dropout_dense_{i+1}'))\n",
    "\n",
    "        if ENABLE_BATCH_NORMALIZATION:\n",
    "            head.add(keras.layers.BatchNormalization(name=f'batchnorm_dense_{i+1}'))\n",
    "\n",
    "    # Softmax layer\n",
    "    head.add(keras.layers.Dense(\n",
    "        len(class_names),\n",
    "        activation='softmax',\n",
    "        name='dense_softmax'\n",
    "    ))\n",
    "\n",
    "    # Combine the model parts\n",
    "    model = keras.Sequential([base_model, head], name=model_name)\n",
    "\n",
    "    # Show a summary of the model\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "source": [
    "Alternatively we can load a pretrained model, with weights, and optionally unfreeze layers for finetuning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, epoch):\n",
    "    with open(models_dir.joinpath(model_name, 'model.json')) as f:\n",
    "        # Load pre-saved configuration\n",
    "        model = keras.models.model_from_json(f.read())\n",
    "\n",
    "        # Load weights from checkpoint\n",
    "        model.load_weights(str(models_dir.joinpath(model_name, 'ckpts', f'ckpt_{epoch:02d}.hdf5')))\n",
    "\n",
    "        # Unfreeze layers for finetuning\n",
    "        if True:\n",
    "            for layer in model.layers[0].layers[17:]:\n",
    "                layer.trainable = True\n",
    "\n",
    "        model.summary()\n",
    "        return model"
   ]
  },
  {
   "source": [
    "Choose loss, learning rate, and optimizer. Then compile the model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    # We are using a categorical (i.e. one-hot output)\n",
    "    loss = keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "    # Learning rate can be adjusted\n",
    "    learning_rate = 1e-5\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    metrics = ['accuracy']\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics\n",
    "    )"
   ]
  },
  {
   "source": [
    "## Training\n",
    "\n",
    "Fit the model to data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 30\n",
    "RUN_SUFFIX = '_step1'\n",
    "\n",
    "# Start with a fresh model or load a pretrained one\n",
    "model = gen_model('Mask_vgg16_gap_1')\n",
    "# model = load_model('Mask_vgg16_gap_1' + '_step2', epoch=30)\n",
    "compile_model(model)\n",
    "\n",
    "model_dir = models_dir.joinpath(model.name + RUN_SUFFIX)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    steps_per_epoch=len(train_gen),\n",
    "    validation_steps=len(valid_gen),\n",
    "    callbacks=gen_callbacks(model.name + RUN_SUFFIX)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model configuration to file\n",
    "with open(model_dir.joinpath('model.json'), 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "\n",
    "# Save history to file\n",
    "df = pd.DataFrame(history.history)\n",
    "df.to_csv(model_dir.joinpath('metrics.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].summary()\n",
    "for i, layer in enumerate(model.layers[0].layers):\n",
    "    print(i, layer.name, layer.trainable)"
   ]
  },
  {
   "source": [
    "## Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We might want to load the best weights from the previous run, according to the validation loss"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(str(models_dir.joinpath(model.name + '_step3', 'ckpts', 'ckpt_03.hdf5')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(labels, predictions):\n",
    "    # Compute the confusion matrix\n",
    "    cm = tf.math.confusion_matrix(labels, predictions).numpy()\n",
    "\n",
    "    # Draw the figure\n",
    "    figure = plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    # Normalize the confusion matrix.\n",
    "    cmn = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "    # Use white text if squares are dark; otherwise black.\n",
    "    threshold = cmn.max() / 2.\n",
    "    for i, j in [(i,j) for i in range(cmn.shape[0]) for j in range(cmn.shape[1])]:\n",
    "        color = \"white\" if cmn[i, j] > threshold else \"black\"\n",
    "        plt.text(j, i, cmn[i, j], horizontalalignment=\"center\", color=color)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    return figure\n",
    "\n",
    "predictions = model.predict(valid_gen)\n",
    "pred_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = valid_gen.classes\n",
    "\n",
    "figure = plot_confusion_matrix(true_classes, pred_classes)"
   ]
  },
  {
   "source": [
    "## Prediction\n",
    "\n",
    "Perform a batch prediction on the test dataset, then save the results to a file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = dataset_dir.joinpath('test')\n",
    "\n",
    "img_paths = list(test_dir.iterdir())\n",
    "\n",
    "def load_preprocess(img_path):\n",
    "    img = keras.preprocessing.image.load_img(img_path).resize(TARGET_SIZE)\n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "    img = preprocessing_function(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "img_batch = np.array([load_preprocess(p) for p in img_paths])\n",
    "predictions = map(np.argmax, model.predict(img_batch))\n",
    "\n",
    "results = pd.DataFrame({'Id': map(lambda p: p.name, img_paths), 'Category': predictions})\n",
    "\n",
    "results.to_csv(model_dir.joinpath('results.csv'), index=False)"
   ]
  }
 ]
}